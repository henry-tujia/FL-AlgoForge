'''
Main file to set up the FL system and train
Code design inspired by https://github.com/FedML-AI/FedML
'''
# import wandb
import datetime
import pathlib
from torch.multiprocessing import Queue, set_start_method

import torch
import numpy as np
import methods.moon as moon
import methods.fedrs as fedrs
import methods.fedprox as fedprox
import methods.feddecorr as feddecorr
import methods.fedict as fedict
import methods.fedfv as fedfv
import methods.fedbalance as fedbalance
import methods.fedavg as fedavg
import utils.custom_multiprocess as cm
import utils.logger_ as logger
import argparse
import logging
import os
import random
import sys
import time
from collections import defaultdict

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


torch.multiprocessing.set_sharing_strategy('file_system')


def add_args(parser):
    # Training settings
    parser.add_argument('--method', type=str, default='fedmix', metavar='N',
                        help='Options are: fedavg, fedprox, moon, mixup, stochdepth, gradaug, fedalign')

    parser.add_argument('--dataset', type=str,
                        default='cifar10', help="name of dataset")

    parser.add_argument('--partition_method', type=str, default='hetero', metavar='N',
                        help='how to partition the dataset on local clients')

    parser.add_argument('--partition_alpha', type=float, default=0.1, metavar='PA',
                        help='alpha value for Dirichlet distribution partitioning of data(default: 0.5)')

    parser.add_argument('--client_number', type=int, default=100, metavar='NN',
                        help='number of clients in the FL system')

    parser.add_argument('--batch_size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')

    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                        help='learning rate (default: 0.01)')

    parser.add_argument('--wd', help='weight decay parameter;',
                        type=float, default=0.0001)

    parser.add_argument('--epochs', type=int, default=10, metavar='EP',
                        help='how many epochs will be trained locally per round')

    parser.add_argument('--comm_round', type=int, default=200,
                        help='how many rounds of communications are conducted')

    parser.add_argument('--pretrained', action='store_true', default=False,
                        help='test pretrained model')

    parser.add_argument('--save_client', action='store_true', default=False,
                        help='Save client checkpoints each round')

    parser.add_argument('--thread_number', type=int, default=2, metavar='NN',
                        help='number of parallel training threads')

    parser.add_argument('--client_sample', type=float, default=0.1, metavar='MT',
                        help='Fraction of clients to sample')

    parser.add_argument('--local_valid', type=bool, default=False,
                        help='Local validition or not')
# For FedIct
    parser.add_argument('--local_model', type=str, default="lenet",
                        help='Local Model Type')

    parser.add_argument('--weight_method', type=str, default="loss",
                        help='Weight calculation method')

    parser.add_argument('--proxy', type=str, default="",
                        help='Proxy for wandb')

    parser.add_argument('--data_path', type=str, default="",
                        help="""path of all dataset, formed as 'data_path'/cifar10 """)

    args = parser.parse_args()

    return args

# Setup Functions


# def set_random_seed(seed=1):
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     # NOTE: If you want every run to be exactly the same each time
#     # uncomment the following lines
#     # torch.backends.cudnn.deterministic = True
#     # torch.backends.cudnn.benchmark = False


def init_process(q, Client):
    set_random_seed()
    global client
    ci = q.get()
    client = Client(ci[0], ci[1])


def run_clients(received_info):
    try:
        return client.run(received_info)
    except KeyboardInterrupt:
        logging.info('exiting')
        return None


def allocate_clients_to_threads(args):
    mapping_dict = defaultdict(list)
    for round in range(args.comm_round):
        if args.client_sample < 1.0:
            num_clients = int(args.client_number*args.client_sample)
            client_list = random.sample(range(args.client_number), num_clients)
        else:
            num_clients = args.client_number
            client_list = list(range(num_clients))
        if num_clients % args.thread_number == 0 and num_clients > 0:
            clients_per_thread = int(num_clients/args.thread_number)
            for c, t in enumerate(range(0, num_clients, clients_per_thread)):
                idxs = [client_list[x] for x in range(t, t+clients_per_thread)]
                mapping_dict[c].append(idxs)
        else:
            raise ValueError(
                "Sampled client number not divisible by number of threads")
    return mapping_dict


# def init_net():
#     if args.dataset in ("cifar10", "cinic10", "femnist", "svhn", "digits+feature", "office+feature", "covid"):
#         Model = resnet8
#     elif args.dataset == "cifar100":
#         Model = resnet32
#     return Model


if __name__ == "__main__":

    FILE_PATH = pathlib.Path(__file__).absolute().parent

    dt = datetime.datetime.now()

    SAVE_PATH = FILE_PATH / "save" / f"""{dt.isoformat()}"""

    

